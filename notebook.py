# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFcmH-e6JkreDxM0W8KlmJSsbmu_vCL6

Face Mask Detection Images Dataset
- **Nama:** RAHMAT HIDAYAT
- **Email:** mc013d5y1559@student.devacademy.id
- **ID Dicoding:** MC013D5Y1559

## Import Semua Packages/Library yang Digunakan
"""

!pip install kaggle
!pip install tensorflow
!pip install tensorflowjs

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.layers import Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.layers import BatchNormalization, Dropout, Dense, GlobalAveragePooling2D
from PIL import Image

# Aktifkan Mixed Precision
tf.keras.mixed_precision.set_global_policy('mixed_float16')

import shutil

# Pindahkan ke direktori yang benar
os.makedirs("/root/.kaggle", exist_ok=True)

# Check if the file already exists in the destination
if not os.path.exists("/root/.kaggle/kaggle.json"):
    shutil.move("kaggle.json", "/root/.kaggle/")
else:
    print("kaggle.json already exists in the destination directory.")

# Ubah izin file agar tidak bisa diakses oleh orang lain
os.chmod("/root/.kaggle/kaggle.json", 600)

!kaggle datasets download -d ashishjangra27/face-mask-12k-images-dataset

import zipfile

# Ekstrak dataset
with zipfile.ZipFile("face-mask-12k-images-dataset.zip", "r") as zip_ref:
    zip_ref.extractall("/content/")

"""## Data Preparation

### Data Loading
"""

# 1. Data Preparation & Loading
dataset_path = "/content/Face Mask Dataset/"
train_dir = os.path.join(dataset_path, "Train")
val_dir = os.path.join(dataset_path, "Validation")
test_dir = os.path.join(dataset_path, "Test")

# Melihat jumlah gambar dan resolusinya
def print_images_resolution(directory):
    unique_sizes = set()
    total_images = 0

    for subdir in os.listdir(directory):
        subdir_path = os.path.join(directory, subdir)
        image_files = os.listdir(subdir_path)
        num_images = len(image_files)
        print(f"{subdir}: {num_images}")
        total_images += num_images

        for img_file in image_files:
            img_path = os.path.join(subdir_path, img_file)
            with Image.open(img_path) as img:
                unique_sizes.add(img.size)

        for size in unique_sizes:
            print(f"- {size}")
        print("---------------")

    print(f"\nTotal: {total_images}")

print_images_resolution(train_dir)

"""### Data Preprocessing

#### Split Dataset
"""

# 2. Data Preprocessing & Augmentation
img_height, img_width = 224, 224
batch_size = 32

train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
val_test_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical'
)
val_data = val_test_datagen.flow_from_directory(
    val_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical'
)
test_data = val_test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

"""## Modelling"""

# 3. Modeling
num_classes = len(train_data.class_indices)
model = models.Sequential([
    Input(shape=(img_height, img_width, 3)),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# 4. Implementasi Callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)

# 5. Training Model
epochs = 30
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=epochs,
    callbacks=[early_stopping, reduce_lr]
)

"""## Evaluasi dan Visualisasi"""

# 6. Evaluasi Model
# Evaluasi pada training set
train_loss, train_acc = model.evaluate(train_data)
print(f"Training Accuracy: {train_acc:.2%}")
print(f"Training Loss: {train_loss:.4f}")

# Evaluasi pada validation set
val_loss, val_acc = model.evaluate(val_data)
print(f"Validation Accuracy: {val_acc:.2%}")
print(f"Validation Loss: {val_loss:.4f}")

# Evaluasi pada testing set
test_loss, test_acc = model.evaluate(test_data)
print(f"Testing Accuracy: {test_acc:.2%}")
print(f"Testing Loss: {test_loss:.4f}")

# 7. Visualisasi Akurasi dan Loss
plt.figure(figsize=(12, 5))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy') # Changed 'validation_accuracy' to 'val_accuracy'
plt.axhline(y=test_acc, color='r', linestyle='--', label='Test Accuracy')
plt.axhline(y=val_acc, color='g', linestyle='--', label='Validation Accuracy')
plt.legend()
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss') # Changed 'validation_loss' to 'val_loss'
plt.axhline(y=test_loss, color='r', linestyle='--', label='Test Loss')
plt.axhline(y=val_loss, color='g', linestyle='--', label='Validation Loss')
plt.legend()
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.tight_layout()
plt.show()

"""## Konversi Model"""

# Pastikan direktori penyimpanan ada
os.makedirs("tfjs_model", exist_ok=True)
os.makedirs("tflite", exist_ok=True)
os.makedirs("saved_model", exist_ok=True)

# Simpan model dalam format SavedModel
model.export("saved_model")

# Konversi ke TF-Lite
converter = tf.lite.TFLiteConverter.from_saved_model("saved_model")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

try:
    tflite_model = converter.convert()
    with open("tflite/model.tflite", "wb") as f:
        f.write(tflite_model)
    print("Konversi ke TFLite berhasil!")
except Exception as e:
    print(f"Gagal mengonversi ke TFLite: {e}")

# Simpan label untuk model TF-Lite
class_labels = list(train_data.class_indices.keys())
with open("tflite/label.txt", "w") as f:
    for label in class_labels:
        f.write(label + "\n")

# Konversi ke TensorFlow.js
!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model saved_model tfjs_model

import subprocess

try:
    subprocess.run([
        "tensorflowjs_converter",
        "--input_format=tf_saved_model",
        "--output_format=tfjs_graph_model",
        "saved_model",
        "tfjs_model"
    ], check=True)
    print("Konversi ke TensorFlow.js berhasil!")
except subprocess.CalledProcessError as e:
    print(f"Gagal mengonversi ke TensorFlow.js: {e}")

print("Proses konversi selesai!")

"""## Inference (Optional)"""

# 9. Inference (Menggunakan TF-Lite)
# Label kelas
class_labels = ["With Mask", "Without Mask"]

def predict_tflite(image_path, model_path='tflite/model.tflite'):
    # Load model TFLite
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    # Ambil detail input & output
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Ambil ukuran input model
    img_height, img_width = input_details[0]['shape'][1:3]

    # Load dan preprocess gambar
    img = Image.open(image_path).resize((img_height, img_width))
    img_array = np.array(img, dtype=np.float32) / 255.0
    img_array = np.expand_dims(img_array, axis=0)  # Tambahkan dimensi batch

    # Set input model
    interpreter.set_tensor(input_details[0]['index'], img_array)
    interpreter.invoke()

    # Ambil hasil prediksi
    predictions = interpreter.get_tensor(output_details[0]['index'])[0]

    # Dapatkan kelas dengan probabilitas tertinggi
    predicted_class = np.argmax(predictions)
    confidence = predictions[predicted_class] * 100

    # Tampilkan hasil prediksi
    print(f"\n=== HASIL PREDIKSI ===")
    for i, prob in enumerate(predictions):
        print(f"{class_labels[i]}: {prob:.4f}")

    print(f"\nKelas Terpilih: {class_labels[predicted_class]} ({confidence:.2f}%)")

    return class_labels[predicted_class], confidence

# Path gambar yang akan diuji
image_path = "/content/sample.jpeg"

# Jalankan prediksi
predicted_label, confidence = predict_tflite(image_path)